{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE for this notebook:\n",
    "1. use pdfplumber to parse pdf file to avoid parsing word by word error\n",
    "2. Add feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import gc\n",
    "import pdfplumber\n",
    "import evaluate\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_markdown(json_string: str) -> dict:\n",
    "    try:\n",
    "        # Try to find JSON string within first and last triple backticks\n",
    "        if json_string[3:13].lower() == \"typescript\":\n",
    "            json_string = json_string.replace(json_string[3:13], \"\",1)\n",
    "        \n",
    "        if 'JSON_OUTPUT_ACCORDING_TO_RESUME_DATA_SCHEMA' in json_string:\n",
    "            json_string = json_string.replace(\"JSON_OUTPUT_ACCORDING_TO_RESUME_DATA_SCHEMA\", \"\",1)\n",
    "        \n",
    "        if json_string[3:7].lower() == \"json\":\n",
    "            json_string = json_string.replace(json_string[3:7], \"\",1)\n",
    "        \n",
    "\n",
    "        # match = re.search(r\"\"\"```*\n",
    "        #                     (?:json)?\n",
    "        #                     (.*)```\"\"\", json_string, flags=re.DOTALL|re.VERBOSE)\n",
    "\n",
    "        # # If no match found, assume the entire string is a JSON string\n",
    "        # if match is None:\n",
    "        #     json_str = json_string\n",
    "        # else:\n",
    "        #     # If match found, use the content within the backticks\n",
    "        #     json_str = match.group(1)\n",
    "\n",
    "        # # Strip whitespace and newlines from the start and end\n",
    "        # json_str = json_str.strip()\n",
    "\n",
    "        # # Parse the JSON string into a Python dictionary while allowing control characters by setting strict to False\n",
    "        # parsed = json.loads(json_str)\n",
    "        parser = JsonOutputParser()\n",
    "        parsed = parser.parse(json_string)\n",
    "\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def get_prompt(system_prompt_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads the content of the file at the given system_prompt_path and returns it as a string.\n",
    "\n",
    "        Args:\n",
    "            system_prompt_path (str): The path to the system prompt file.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the file as a string.\n",
    "        \"\"\"\n",
    "        with open(system_prompt_path, encoding=\"utf-8\") as file:\n",
    "            return file.read().strip() + \"\\n\"\n",
    "        \n",
    "def extract_text(pdf_path: str):\n",
    "    resume_text = \"\" \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(len(pdf.pages)):\n",
    "            resume_text  += pdf.pages[page_num].extract_text()\n",
    "        return resume_text\n",
    "\n",
    "    # resume_text = \"\"\n",
    "    # with open(pdf_path, 'rb') as file:\n",
    "    #     pdf_reader = PyPDF2.PdfReader(file)\n",
    "    #     num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    #     for page_num in range(num_pages):\n",
    "    #         page = pdf_reader.pages[page_num]\n",
    "    #         text = page.extract_text().split(\"\\n\")\n",
    "\n",
    "    #         # Remove Unicode characters from each line\n",
    "    #         cleaned_text = [re.sub(r'[^\\x00-\\x7F]+', '', line) for line in text]\n",
    "\n",
    "    #         # Join the lines into a single string\n",
    "    #         cleaned_text_string = '\\n'.join(cleaned_text)\n",
    "    #         resume_text += cleaned_text_string\n",
    "        \n",
    "    #     return resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education, Skills, Work Experience, Projects, Certifications, Achievements\n",
    "#\"/home/gabe/workspace/resume_work/job-llm/zlm/prompts/resume-extractor.txt\"\n",
    "system_prompt_path_heading_info = \"resume-extractor_heading_info.txt\"\n",
    "system_prompt_path_education = \"resume-extractor_education.txt\" \n",
    "system_prompt_path_skills = \"resume-extractor_skills.txt\" \n",
    "system_prompt_path_work_experience = \"resume-extractor_work_experience.txt\" \n",
    "system_prompt_path_projects = \"resume-extractor_projects.txt\"\n",
    "system_prompt_path_certifications = \"resume-extractor_certifications.txt\"\n",
    "system_prompt_path_achievements = \"resume-extractor_achievements.txt\"\n",
    "\n",
    "system_prompt_heading_info = get_prompt(system_prompt_path_heading_info)\n",
    "system_prompt_education = get_prompt(system_prompt_path_education)\n",
    "system_prompt_skills = get_prompt(system_prompt_path_skills)\n",
    "system_prompt_work_experience = get_prompt(system_prompt_path_work_experience)\n",
    "system_prompt_projects = get_prompt(system_prompt_path_projects)\n",
    "system_prompt_certifications = get_prompt(system_prompt_path_certifications)\n",
    "system_prompt_achievements = get_prompt(system_prompt_path_achievements)\n",
    "\n",
    "\n",
    "pdf_path_demo = \"/home/gabe/workspace/resume_work/job-llm/zlm/demo_data/user_resume.pdf\"\n",
    "pdf_path_mine = \"/home/gabe/workspace/resume_work/job-llm/zlm/demo_data/my_resume.pdf\"\n",
    "resume_text_demo = extract_text(pdf_path_demo)\n",
    "resume_text_mine = extract_text(pdf_path_mine)\n",
    "resume_to_use = resume_text_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score between SKILLS and Skills: 0.5, line number: 3\n",
      "score between WORK EXPERIENCE and Work Experience: 0.9375, line number: 5\n",
      "score between PROJECTS and Projects: 0.5, line number: 25\n",
      "score between EDUCATION and Education: 0.5, line number: 35\n",
      "{3: 'Skills', 5: 'Work Experience', 25: 'Projects', 35: 'Education'}\n",
      "['SKILLS', 'WORK EXPERIENCE', 'PROJECTS', 'EDUCATION']\n"
     ]
    }
   ],
   "source": [
    "lines = resume_to_use.split(\"\\n\")\n",
    "targets_dict = {\n",
    "    \"Education\": [\"Education\", \"Educations\", \"educational background\", \"education and training'\"],\n",
    "    \"Skills\": [\"Skills\", \"Skill\"],\n",
    "    \"Work Experience\": [\"Work\", \"Work Experience\", \"Experience\"],\n",
    "    \"Projects\": [\"Projects\", \"Project\"],\n",
    "    \"Certifications\": [\"Certification\", \"Certifications\"],\n",
    "    \"Achievements\": [\"Achievements\"]\n",
    "}\n",
    "section_line_number = {}\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "for line in lines:\n",
    "    for target_name, target_set in targets_dict.items():\n",
    "        best_score = 0\n",
    "        name = \"\"\n",
    "        for target in target_set:\n",
    "            line_lower = line.lower()\n",
    "            target_lower = target.lower()\n",
    "            #print(line_lower, target)\n",
    "            results = meteor.compute(predictions=[target_lower], references=[line_lower])\n",
    "            # use the difference in length as a penalty\n",
    "            penalty = abs(len(line_lower.split()) - len(target_lower.split()))\n",
    "            penalty = 1 if penalty == 0 else penalty\n",
    "            score = results[\"meteor\"] / penalty\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                name = target\n",
    "        if best_score > 0.18:\n",
    "            print(\"score between {} and {}: {}, line number: {}\".format(line, name, best_score, lines.index(line)))\n",
    "            section_line_number[lines.index(line)] = target_name\n",
    "print(section_line_number)\n",
    "print([lines[i] for i in section_line_number.keys()])\n",
    "index_list = list(section_line_number.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AMEY SADANAND BHILEGAONKAR\\n'\n",
      " '(cid:211) 480-616-3980  ameybhilegaonkar3@gmail.com (cid:135) '\n",
      " 'github.com/ameygoes fl linkedin.com/in/amey-bhilegaonkar\\n'\n",
      " 'Withover3+yearsofexperienceasaDataEngineerspecializinginETLdatapipelinesforlarge-scaledistributeddatabase\\n'\n",
      " 'systems, I bring both expertise and enthusiasm for handling complex data '\n",
      " 'challenges to your team.\\n'\n",
      " 'WORK EXPERIENCE\\n'\n",
      " 'BigCommerce Austin, Texas\\n'\n",
      " 'Data Science Intern June 2023 - August 2023\\n'\n",
      " '· Designed and managed a large-scale Snowflake data retrieval pipeline for '\n",
      " 'efficient data warehousing.\\n'\n",
      " '· Implemented logistic regression, and predictive models, improving customer '\n",
      " 'retention prediction accuracy by 12%.\\n'\n",
      " '· Collaborated with data infrastructure teams to ensure data availability '\n",
      " 'and resolve data-related issues.\\n'\n",
      " '· Leveraged advanced data-mining techniques to process and analyze millions '\n",
      " 'of data points, extracting critical features for\\n'\n",
      " 'search indexing and ranking.\\n'\n",
      " 'Publicis Sapient Bangalore, India\\n'\n",
      " 'Data Engineer - II June 2019 - July 2022\\n'\n",
      " '· Engineered complex ETL pipelines using Apache Spark, optimizing data '\n",
      " 'extraction, transformation, and loading from\\n'\n",
      " 'diverse sources, such as Redshift, S3, Kinesis Streams, and Kafka.\\n'\n",
      " '· Utilized distributed computing frameworks such as Apache Spark to manage '\n",
      " 'large-scale data processing tasks, enhancing\\n'\n",
      " 'performance and resource utilization by 15%.\\n'\n",
      " '· Independently designed, and built database tools and scripts to simplify '\n",
      " 'and automate reduced operation toil by 15%.\\n'\n",
      " '· Improved database performance by optimizing queries and tuning indexes, '\n",
      " 'resulting in 20% reduction in query execution.\\n'\n",
      " '· Revamped and maintained real-time data streaming solutions with Apache '\n",
      " 'Spark, and GCP Cloud Run in large-scale\\n'\n",
      " 'infrastructure markets with over 30 million daily customer transactions, '\n",
      " 'resulting in a 15% revenue increase.\\n'\n",
      " '· Utilized AWS Lambda, EC2, and SNS for scheduled data transformations, '\n",
      " 'cutting pipeline run-time by 2 hours weekly.\\n'\n",
      " 'PROJECTS\\n'\n",
      " 'Search Engine for All file types - Opportunity Hackathon - Meta Sponsored\\n'\n",
      " '· Spearheaded Elasticsearch implementation for blazing-fast search '\n",
      " 'responses, with millisecond response times.\\n'\n",
      " '· Converted and stored every file type data as vector embeddings, ensuring '\n",
      " 'low-latency search capabilities.\\n'\n",
      " '· Led Python FAST API development, providing efficient data access and '\n",
      " 'retrieval.\\n'\n",
      " '· Used Machine Learning techniques such as BERT, OCR, ResNet50, and Image '\n",
      " 'Captioning to parse Image features.\\n'\n",
      " '· Collaborated effectively with team members, optimizing task distribution '\n",
      " 'for streamlined project completion.\\n'\n",
      " 'Scalable Data Processing Pipeline - Neo4J, Docker, Kafka and Minikube\\n'\n",
      " '· Designed and implemented a highly scalable and available data processing '\n",
      " 'pipeline using Kubernetes, Kafka, Docker, Neo4j.\\n'\n",
      " '· Orchestrated the setup of Kafka and Apache Zookeeper using Minikube, a '\n",
      " 'lightweight Kubernetes implementation.\\n'\n",
      " '· Streamlined data ingestion, and processing into Neo4j, applying PageRank '\n",
      " 'and BFS for graph-based data exploration.\\n'\n",
      " 'Email Automation Marketing Tool\\n'\n",
      " '· Initiated and completed the development of a robust email automation '\n",
      " 'project, streamlining job application outreach and net-\\n'\n",
      " 'workingeffortswithindividualsofsimilarinterests. '\n",
      " 'employedRESTFulAPIintegrationtoacquireandmanageacomprehensive\\n'\n",
      " 'contact database from CRM tools, demonstrating prowess in full-stack '\n",
      " 'development and API design.\\n'\n",
      " 'Speech Emotion Detection\\n'\n",
      " '· Researched and optimized existing emotion detection approaches by '\n",
      " 'combining CNN and LSTM networks.\\n'\n",
      " '· Discovered emotion-affecting attributes in voice by analyzing audio signal '\n",
      " 'features (MFCC, ZCR, Pitch, Chroma).\\n'\n",
      " 'EDUCATION\\n'\n",
      " 'Arizona State University, Tempe, USA August 2022 - May 2024\\n'\n",
      " 'Masters of Science in Computer Science (GPA: 4/4)\\n'\n",
      " 'Pune Institute of Computer Technology, Pune, India July 2015 - May 2019\\n'\n",
      " 'Bachelors of Engineering in Electronics and Telecommunications (GPA: '\n",
      " '8.78/10)\\n'\n",
      " 'TECHNICAL SKILLS\\n'\n",
      " 'Programming Languages Python, Unix / Linux Scripting\\n'\n",
      " 'Cloud Platforms & Databases SQL, GCP, AWS, Big Query, Cassandra\\n'\n",
      " 'Data Engineering SnowFlake, Airflow, Spark, Kafka, Pandas, Tableau, D3.js\\n'\n",
      " 'DevOps / SRE CI/CD, Git, Jenkins, Docker, Kubernetes\\n'\n",
      " 'Certified Google Cloud Platform Associate Cloud Engineer.')\n",
      "Gabriel Chen (Shou-Zhong)\n",
      "(+1)647-979-9461 szgabrielchen@gmail.com linkedin.com/in/gabrielchen65\n",
      "4yearsofMachineLearningEngineerexperience,ledinfraredfacerecognitionproject,deliveredhigh-impactproduct.\n",
      "SKILLS\n",
      "NLP|LLM|Python|PyTorch|TensorFlow|Ubuntu|GitHub|SQL(MySQL)|ComputerVision\n",
      "WORK EXPERIENCE\n",
      "Egis Technology 2019May–2022June\n",
      "Machine Learning Engineer (Infrared Face Recognition Project with Python, Pytorch)\n",
      "● Project lead since October 2020. The product has been in mass production since October 2021.\n",
      "This product drove $5 million in revenue and fostered the following partnership with the client.\n",
      "● Analyzedrootcausesofpredictionerrors,andmanageddatacollectionfromin-houseandoutsourcedteams\n",
      "● Fine-tunedmodelwithlarge-scaleimagedatasetfromdifferentsourceswithdistributedcomputingandscheduler\n",
      "● Proposedandimplementeddatapipelinerefinementstoreduce10%falserejectionrate\n",
      "● Reduced5%falserejectionrateusingdataaugmentationwithsimulatedsunlightpattern\n",
      "● ProposedandimplementedanensembleAnti-Spoofingmethodtoreducethespoofingacceptanceratetolower\n",
      "than1%whilemaintainingthelowfalserejectionrate\n",
      "● Createdadataaugmentationformouth/nosedetectionwithminimumtrainingdataforeffectivemaskdetection\n",
      "duringthepandemicera\n",
      "Machine Learning Engineer (Edge AI Chip Development Project)\n",
      "● Evaluatedandredesignedthearchitectureofmodels(segmentation,super-resolution)usingPytorchtoensurethe\n",
      "peakmemoryusageandFLOPSarecompatiblewiththein-houseedgeAIintegratedcircuitdesign\n",
      "Theia Tech 2018Oct–2019April\n",
      "Machine Learning Engineer (Glasses-free 3D Display Project)\n",
      "● Capturedthelocationofon-screendisplaytextwiththeobjectdetectionmethodtoavoidabnormal3Deffectson\n",
      "textandgenerateabetteruserexperience\n",
      "PROJECTS\n",
      "Clickbait Spoiling: Aimed to save users' time by spoiling tedious articles with sensational titles. GitHub\n",
      "● Devisedandimplementedacomprehensivepipelinetocountervariousclickbaittypes,boostingperformanceby9%\n",
      "● UtilizedHuggingFacetokenizationandtransformermodels(RoBERTa,T5)onKagglecloudandSageMaker\n",
      "● Fine-tunedlargermodelswithLoRA,furtherimprovingperformanceby15%(meteorscore).\n",
      "● EmployedGPT-3.5turboAPIasprojectbaseline.\n",
      "Automatic Resume Tailoring(Open-sourcecontribution):GitHub\n",
      "● AddingsupportforthelocalLLMoptions(MistralwithHuggingFaceorOllama)andspeedingupthepipelineby5%\n",
      "● Optimizingperformancewiththefeedbackloop.\n",
      "ChatGPT clone chatbot: DeployedChatGPT-likeappwithOpenAIAPIandStreamlitonAWSEC2.Link\n",
      "EDUCATION\n",
      "Master of Electrical and Computer Engineering, University of Waterloo(3.75/4) 2022Sep–2023Dec\n",
      "● Used-CarDatabaseProject:DesignedthecompleteERdiagramfromscratchandusedSQL(MySQL)toimplement\n",
      "● Adaptivewebtrafficcircuitbreaking:DeployedmicroserviceswithKubernetes,Prometheus,andDocker\n",
      "● AndroidAppDevelopmentProject:AsascrummasterinAgiledevelopmentwithJira,appliedtheCI/CD\n",
      "Master of Electrical Engineering, National Tsing Hua University (NTHU) (3.84/4.3) 2015Sep–2018Feb\n",
      "● Publicationasthesecondauthor:“RecognitionfromHandCameras:ARevisitwithDeepLearning”,ECCV2016\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(resume_text_demo)\n",
    "print(resume_text_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heading_info': ['Gabriel Chen (Shou-Zhong)', '(+1)647-979-9461 szgabrielchen@gmail.com linkedin.com/in/gabrielchen65', '4yearsofMachineLearningEngineerexperience,ledinfraredfacerecognitionproject,deliveredhigh-impactproduct.'], 'Skills': ['SKILLS', 'NLP|LLM|Python|PyTorch|TensorFlow|Ubuntu|GitHub|SQL(MySQL)|ComputerVision'], 'Work Experience': ['WORK EXPERIENCE', 'Egis Technology 2019May–2022June', 'Machine Learning Engineer (Infrared Face Recognition Project with Python, Pytorch)', '● Project lead since October 2020. The product has been in mass production since October 2021.', 'This product drove $5 million in revenue and fostered the following partnership with the client.', '● Analyzedrootcausesofpredictionerrors,andmanageddatacollectionfromin-houseandoutsourcedteams', '● Fine-tunedmodelwithlarge-scaleimagedatasetfromdifferentsourceswithdistributedcomputingandscheduler', '● Proposedandimplementeddatapipelinerefinementstoreduce10%falserejectionrate', '● Reduced5%falserejectionrateusingdataaugmentationwithsimulatedsunlightpattern', '● ProposedandimplementedanensembleAnti-Spoofingmethodtoreducethespoofingacceptanceratetolower', 'than1%whilemaintainingthelowfalserejectionrate', '● Createdadataaugmentationformouth/nosedetectionwithminimumtrainingdataforeffectivemaskdetection', 'duringthepandemicera', 'Machine Learning Engineer (Edge AI Chip Development Project)', '● Evaluatedandredesignedthearchitectureofmodels(segmentation,super-resolution)usingPytorchtoensurethe', 'peakmemoryusageandFLOPSarecompatiblewiththein-houseedgeAIintegratedcircuitdesign', 'Theia Tech 2018Oct–2019April', 'Machine Learning Engineer (Glasses-free 3D Display Project)', '● Capturedthelocationofon-screendisplaytextwiththeobjectdetectionmethodtoavoidabnormal3Deffectson', 'textandgenerateabetteruserexperience'], 'Projects': ['PROJECTS', \"Clickbait Spoiling: Aimed to save users' time by spoiling tedious articles with sensational titles. GitHub\", '● Devisedandimplementedacomprehensivepipelinetocountervariousclickbaittypes,boostingperformanceby9%', '● UtilizedHuggingFacetokenizationandtransformermodels(RoBERTa,T5)onKagglecloudandSageMaker', '● Fine-tunedlargermodelswithLoRA,furtherimprovingperformanceby15%(meteorscore).', '● EmployedGPT-3.5turboAPIasprojectbaseline.', 'Automatic Resume Tailoring(Open-sourcecontribution):GitHub', '● AddingsupportforthelocalLLMoptions(MistralwithHuggingFaceorOllama)andspeedingupthepipelineby5%', '● Optimizingperformancewiththefeedbackloop.', 'ChatGPT clone chatbot: DeployedChatGPT-likeappwithOpenAIAPIandStreamlitonAWSEC2.Link'], 'Education': (['EDUCATION', 'Master of Electrical and Computer Engineering, University of Waterloo(3.75/4) 2022Sep–2023Dec', '● Used-CarDatabaseProject:DesignedthecompleteERdiagramfromscratchandusedSQL(MySQL)toimplement', '● Adaptivewebtrafficcircuitbreaking:DeployedmicroserviceswithKubernetes,Prometheus,andDocker', '● AndroidAppDevelopmentProject:AsascrummasterinAgiledevelopmentwithJira,appliedtheCI/CD', 'Master of Electrical Engineering, National Tsing Hua University (NTHU) (3.84/4.3) 2015Sep–2018Feb', '● Publicationasthesecondauthor:“RecognitionfromHandCameras:ARevisitwithDeepLearning”,ECCV2016'],)}\n"
     ]
    }
   ],
   "source": [
    "sections = {\n",
    "    \"heading_info\": resume_to_use.split('\\n')[:index_list[0]],\n",
    "}\n",
    "\n",
    "for idx, (start_line, section_name) in enumerate(section_line_number.items()):\n",
    "    if idx < len(index_list) - 1:\n",
    "        sections[section_name] = resume_to_use.split('\\n')[index_list[idx]:index_list[idx+1]]\n",
    "    else:\n",
    "        sections[section_name] = resume_to_use.split('\\n')[index_list[idx]:],\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mistral:\n",
    "    def __init__(self):\n",
    "        \n",
    "        model_id= \"mistralai/Mistral-7B-Instruct-v0.2\" # \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        quantization_config = transformers.BitsAndBytesConfig(\n",
    "                                load_in_8bit=True,\n",
    "                                bnb_8bit_compute_dtype=torch.bfloat16\n",
    "                            )        \n",
    "\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto',\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        self.generate_text = transformers.pipeline(\n",
    "            model=model, tokenizer=tokenizer,\n",
    "            return_full_text=False,  # if using langchain set True\n",
    "            task=\"text-generation\",\n",
    "            do_sample=False,\n",
    "            max_new_tokens=4000,  # max number of tokens to generate in the output\n",
    "            repetition_penalty=1.15,  # if output begins repeating increase\n",
    "        )         \n",
    "\n",
    "    def get_response(self, system_prompt, prompt_text, expecting_longer_output=False, need_json_output=False, feedback=False):\n",
    "        # Special format required by the Mistral Instruct Chat Model \n",
    "        # where we can use system messages to provide more context about the task\n",
    "        prompt = f'<s> [INST] {system_prompt} {prompt_text} [/INST]'\n",
    "\n",
    "        response = self.generate_text(prompt)[0][\"generated_text\"]\n",
    "\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "        if need_json_output:\n",
    "            parsed = parse_json_markdown(response)\n",
    "            if parsed:\n",
    "                # for debugging\n",
    "                with open(\"/home/gabe/workspace/resume_work/job-llm/lab/resume_parsed.json\", \"w\") as f:\n",
    "                    json.dump(parsed, f, indent=4)\n",
    "                return parsed, response\n",
    "            else:\n",
    "                if feedback:\n",
    "                    print(\"Parsing failed, giving feedback to the model and run again...\")\n",
    "                    prompt += f'{prompt} {response} </s> [INST] Your previous output is not a valid JSON format, try to fix that [/INST]'\n",
    "                    response = self.generate_text(prompt)[0][\"generated_text\"]\n",
    "                    parsed = parse_json_markdown(response)\n",
    "                    return parsed, response\n",
    "                else:\n",
    "                    print(\"parsing failed. Returning raw response\")\n",
    "                    return None, response\n",
    "        else:\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = Mistral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# heading_info, Education, Skills, Work Experience, Projects, Certifications, Achievements\n",
    "resume_json_heading_info, response_heading_info = llm.get_response(\n",
    "                system_prompt=system_prompt_heading_info, \n",
    "                prompt_text=sections[\"heading_info\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# heading_info, Education, Skills, Work Experience, Projects, Certifications, Achievements\n",
    "resume_json_education, response_education = llm.get_response(\n",
    "                system_prompt=system_prompt_education, \n",
    "                prompt_text=sections[\"Education\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resume_json_skills, response_skills = llm.get_response(\n",
    "                system_prompt=system_prompt_skills, \n",
    "                prompt_text=sections[\"Skills\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resume_json_work_experience, response_work_experience = llm.get_response(\n",
    "                system_prompt=system_prompt_work_experience, \n",
    "                prompt_text=sections[\"Work Experience\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "resume_json_projects, response_projects = llm.get_response(\n",
    "                system_prompt=system_prompt_projects, \n",
    "                prompt_text=sections[\"Projects\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Certifications'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Certifications, Achievements\u001b[39;00m\n\u001b[1;32m      2\u001b[0m resume_json_certifications, response_certifications \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m      3\u001b[0m                 system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt_certifications, \n\u001b[0;32m----> 4\u001b[0m                 prompt_text\u001b[38;5;241m=\u001b[39m\u001b[43msections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCertifications\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, need_json_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Certifications'"
     ]
    }
   ],
   "source": [
    "# Certifications, Achievements\n",
    "resume_json_certifications, response_certifications = llm.get_response(\n",
    "                system_prompt=system_prompt_certifications, \n",
    "                prompt_text=sections[\"Certifications\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Certifications, Achievements\n",
    "resume_json_achievements, response_achievements = llm.get_response(\n",
    "                system_prompt=system_prompt_achievements, \n",
    "                prompt_text=sections[\"Achievements\"], need_json_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the JSON files from each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_all = {\n",
    "  \"name\": resume_json_heading_info[\"name\"],\n",
    "  \"summary\": resume_json_heading_info[\"summary\"],\n",
    "  \"phone\": resume_json_heading_info[\"phone\"],\n",
    "  \"email\": resume_json_heading_info[\"email\"],\n",
    "  \"media\": {\n",
    "    \"linkedin\": resume_json_heading_info[\"linkedin\"],\n",
    "    \"github\": resume_json_heading_info[\"github\"] if \"github\" in resume_json_heading_info else \"\",\n",
    "    \"devpost\": resume_json_heading_info[\"devpost\"] if \"devpost\" in resume_json_heading_info else \"\",\n",
    "    \"medium\": resume_json_heading_info[\"medium\"] if \"medium\" in resume_json_heading_info else \"\",\n",
    "    \"leetcode\": resume_json_heading_info[\"leetcode\"] if \"leetcode\" in resume_json_heading_info else \"\",\n",
    "    \"dagshub\": resume_json_heading_info[\"dagshub\"] if \"dagshub\" in resume_json_heading_info else \"\",\n",
    "    \"kaggle\": resume_json_heading_info[\"kaggle\"] if \"kaggle\" in resume_json_heading_info else \"\",\n",
    "    \"instagram\": resume_json_heading_info[\"instagram\"] if \"instagram\" in resume_json_heading_info else \"\",\n",
    "  },\n",
    "  \"education\": resume_json_education,\n",
    "  \"skills\": resume_json_skills,\n",
    "  \"work_experience\": resume_json_work_experience,\n",
    "  \"projects\": resume_json_projects,\n",
    "  \"certifications\": [],\n",
    "  \"achievements\": []\n",
    "}\n",
    "\n",
    "json.dump(resume_all, open(\"resume_all_generate_by_section.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "def cosine_similarity(document1: str, document2: str) -> float:\n",
    "    \"\"\"Calculate the cosine similarity between two documents.\n",
    "\n",
    "    Args:\n",
    "        document1 (str): The first document.\n",
    "        document2 (str): The second document.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two documents.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the documents into TF-IDF vectors\n",
    "    vectors = vectorizer.fit_transform([document1, document2])\n",
    "\n",
    "    cosine_similarity_score = pairwise.cosine_similarity(vectors[0], vectors[1])\n",
    "    # Calculate the cosine similarity between the two vectors\n",
    "    # cosine_similarity = np.dot(vectors[0], vectors[1].T) / (np.linalg.norm(vectors[0].toarray()) * np.linalg.norm(vectors[1].toarray()))\n",
    "\n",
    "    return cosine_similarity_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6308746705618988\n",
      "1.0000000000000009\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "generated = json.dumps(resume_all)\n",
    "with open(\"user_profile.json\") as f:\n",
    "    ground_truth = json.dumps(json.load(f))\n",
    "score = cosine_similarity(generated, ground_truth)\n",
    "print(score)\n",
    "score = cosine_similarity(generated, generated)\n",
    "print(score)\n",
    "score = cosine_similarity(ground_truth, ground_truth)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9450426644246636\n",
      "0.9999999999999998\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "generated = json.dumps(resume_all)\n",
    "with open(\"my_resume_ground_truth.json\") as f:\n",
    "    ground_truth = json.dumps(json.load(f))\n",
    "score = cosine_similarity(generated, ground_truth)\n",
    "print(score)\n",
    "score = cosine_similarity(generated, generated)\n",
    "print(score)\n",
    "score = cosine_similarity(ground_truth, ground_truth)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect() # Python thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import json\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import gc\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! task is not default parameter.\n",
      "                task was transferred to model_kwargs.\n",
      "                Please confirm that task is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! do_sample is not default parameter.\n",
      "                do_sample was transferred to model_kwargs.\n",
      "                Please confirm that do_sample is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! repetition_penalty is not default parameter.\n",
      "                repetition_penalty was transferred to model_kwargs.\n",
      "                Please confirm that repetition_penalty is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! attention_dropout is not default parameter.\n",
      "                attention_dropout was transferred to model_kwargs.\n",
      "                Please confirm that attention_dropout is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! eos_token_id is not default parameter.\n",
      "                eos_token_id was transferred to model_kwargs.\n",
      "                Please confirm that eos_token_id is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! bos_token_id is not default parameter.\n",
      "                bos_token_id was transferred to model_kwargs.\n",
      "                Please confirm that bos_token_id is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! hidden_act is not default parameter.\n",
      "                hidden_act was transferred to model_kwargs.\n",
      "                Please confirm that hidden_act is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! hidden_size is not default parameter.\n",
      "                hidden_size was transferred to model_kwargs.\n",
      "                Please confirm that hidden_size is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! initializer_range is not default parameter.\n",
      "                initializer_range was transferred to model_kwargs.\n",
      "                Please confirm that initializer_range is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! intermediate_size is not default parameter.\n",
      "                intermediate_size was transferred to model_kwargs.\n",
      "                Please confirm that intermediate_size is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! max_position_embeddings is not default parameter.\n",
      "                max_position_embeddings was transferred to model_kwargs.\n",
      "                Please confirm that max_position_embeddings is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! model_type is not default parameter.\n",
      "                model_type was transferred to model_kwargs.\n",
      "                Please confirm that model_type is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! num_attention_heads is not default parameter.\n",
      "                num_attention_heads was transferred to model_kwargs.\n",
      "                Please confirm that num_attention_heads is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! num_hidden_layers is not default parameter.\n",
      "                num_hidden_layers was transferred to model_kwargs.\n",
      "                Please confirm that num_hidden_layers is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! num_key_value_heads is not default parameter.\n",
      "                num_key_value_heads was transferred to model_kwargs.\n",
      "                Please confirm that num_key_value_heads is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! rms_norm_eps is not default parameter.\n",
      "                rms_norm_eps was transferred to model_kwargs.\n",
      "                Please confirm that rms_norm_eps is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! rope_theta is not default parameter.\n",
      "                rope_theta was transferred to model_kwargs.\n",
      "                Please confirm that rope_theta is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! tie_word_embeddings is not default parameter.\n",
      "                tie_word_embeddings was transferred to model_kwargs.\n",
      "                Please confirm that tie_word_embeddings is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! torch_dtype is not default parameter.\n",
      "                torch_dtype was transferred to model_kwargs.\n",
      "                Please confirm that torch_dtype is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! transformers_version is not default parameter.\n",
      "                transformers_version was transferred to model_kwargs.\n",
      "                Please confirm that transformers_version is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! use_cache is not default parameter.\n",
      "                use_cache was transferred to model_kwargs.\n",
      "                Please confirm that use_cache is what you intended.\n",
      "  warnings.warn(\n",
      "/home/gabe/miniconda3/envs/resume/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! vocab_size is not default parameter.\n",
      "                vocab_size was transferred to model_kwargs.\n",
      "                Please confirm that vocab_size is what you intended.\n",
      "  warnings.warn(\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] \n",
      "value ::= object | array | string | number | boolean | [n] [u] [l] [l] \n",
      "array ::= [[] ws array_15 []] \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_20 ws \n",
      "boolean ::= boolean_21 ws \n",
      "ws ::= ws_23 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= [-] | \n",
      "number_20 ::= [0-9] number_20 | [0-9] \n",
      "boolean_21 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "ws_22 ::= [ <U+0009><U+000A>] ws \n",
      "ws_23 ::= ws_22 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 4000  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "#model_path = \"/home/gabe/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/gguf/mistral-7b-instruct-v0.2.Q8_0.gguf\"\n",
    "#model_path = \"/home/gabe/workspace/models_library/mixtral-instruct-8x7b-2.34bpw.gguf\" #\"/home/gabe/workspace/llama.cpp/Mistral-7B-Instruct-v0.2.gguf\"\n",
    "model_path = \"/home/gabe/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/gguf/mistral-7b-instruct-v0.2.Q8_0.gguf\"\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    grammar_path=\"./json.gbnf\",\n",
    "    n_ctx=32000, # max number of tokens in the input\n",
    "    rope_freq_base=1e6,\n",
    "    verbose=False,\n",
    "    echo=False,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    max_tokens=4000,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.15,  # if output begins repeating increase\n",
    "    attention_dropout=0.0,\n",
    "    eos_token_id=2,\n",
    "    bos_token_id=1,\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_size=4096,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=14336,\n",
    "    max_position_embeddings=32768,\n",
    "    model_type=\"mistral\",\n",
    "    num_attention_heads=32,\n",
    "    num_hidden_layers=32,\n",
    "    num_key_value_heads=8,\n",
    "    rms_norm_eps=1e-05,\n",
    "    rope_theta=1000000.0,\n",
    "    #sliding_window=null,\n",
    "    tie_word_embeddings=False,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    transformers_version=\"4.36.0\",\n",
    "    use_cache=True,\n",
    "    vocab_size=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s> [INST] {system_prompt_heading_info} {sections[\"heading_info\"]} [/INST]'\n",
    "response = llm.invoke(prompt)\n",
    "resume_json_heading_info = parse_json_markdown(response)\n",
    "#pprint(parsed)\n",
    "#pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s> [INST] {system_prompt_education} {sections[\"Education\"]} [/INST]'\n",
    "response = llm.invoke(prompt)\n",
    "resume_json_education = parse_json_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s> [INST] {system_prompt_skills} {sections[\"Skills\"]} [/INST]'\n",
    "response = llm.invoke(prompt)\n",
    "resume_json_skills = parse_json_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s> [INST] {system_prompt_work_experience} {sections[\"Work Experience\"]} [/INST]'\n",
    "response = llm.invoke(prompt)\n",
    "resume_json_work_experience = parse_json_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s> [INST] {system_prompt_path_projects} {sections[\"Projects\"]} [/INST]'\n",
    "response = llm.invoke(prompt)\n",
    "resume_json_projects = parse_json_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_all_langchain = {\n",
    "  \"name\": resume_json_heading_info[\"name\"],\n",
    "  \"summary\": resume_json_heading_info[\"summary\"],\n",
    "  \"phone\": resume_json_heading_info[\"phone\"],\n",
    "  \"email\": resume_json_heading_info[\"email\"],\n",
    "  \"media\": {\n",
    "    \"linkedin\": resume_json_heading_info[\"linkedin\"] if \"linkedin\" in resume_json_heading_info else \"\",\n",
    "    \"github\": resume_json_heading_info[\"github\"] if \"github\" in resume_json_heading_info else \"\",\n",
    "    \"devpost\": resume_json_heading_info[\"devpost\"] if \"devpost\" in resume_json_heading_info else \"\",\n",
    "    \"medium\": resume_json_heading_info[\"medium\"] if \"medium\" in resume_json_heading_info else \"\",\n",
    "    \"leetcode\": resume_json_heading_info[\"leetcode\"] if \"leetcode\" in resume_json_heading_info else \"\",\n",
    "    \"dagshub\": resume_json_heading_info[\"dagshub\"] if \"dagshub\" in resume_json_heading_info else \"\",\n",
    "    \"kaggle\": resume_json_heading_info[\"kaggle\"] if \"kaggle\" in resume_json_heading_info else \"\",\n",
    "    \"instagram\": resume_json_heading_info[\"instagram\"] if \"instagram\" in resume_json_heading_info else \"\",\n",
    "  },\n",
    "  \"education\": resume_json_education,\n",
    "  \"skills\": resume_json_skills,\n",
    "  \"work_experience\": resume_json_work_experience,\n",
    "  \"projects\": resume_json_projects,\n",
    "  \"certifications\": [],\n",
    "  \"achievements\": []\n",
    "}\n",
    "\n",
    "json.dump(resume_all_langchain, open(\"resume_all_langchain_generate_by_section.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829009171403182\n",
      "0.9999999999999998\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "generated = json.dumps(resume_all_langchain)\n",
    "with open(\"my_resume_ground_truth.json\") as f:\n",
    "    ground_truth = json.dumps(json.load(f))\n",
    "score = cosine_similarity(generated, ground_truth)\n",
    "print(score)\n",
    "score = cosine_similarity(generated, generated)\n",
    "print(score)\n",
    "score = cosine_similarity(ground_truth, ground_truth)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
